{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"private_outputs":true,"provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"MMFOoHOaq96x"},"source":["# Making a Rick Sanchez chat bot for Chai\n","\n","<img src=\"https://i.imgur.com/dtGCncO.png\" width=\"250\">\n","\n","## This version of the notebook is now out of date, the new version can be found [here](https://colab.research.google.com/drive/1o5LxBspm-C28HQvXN-PRQavapDbm5WjG?usp=sharing)\n","\n","We're going to be making a chatbot, based on Microsoft's DialoGPT. I've seen lots of other guides on training chatbots, but I haven't come across one which actually deploys the bot. This tutorial will cover deploying our new bot to [Chai](https://chai.ml/), a platform for creating and interacting with conversational AI's. It will allow us to chat with our bot through a mobile app, from anywhere, at any time. We will also be able to see performance stats and watch it climb the Chai bot leaderboard.\n","\n","Almost all of the code for training this bot was made by [Rostyslav Neskorozhenyi](https://www.linkedin.com/in/slanj) \n","\\(their article can be found [here](https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30)\\). Rostyslav's code has been adapted to suit the tutorial better.\n","\n","The training data has been fetched from this [article](https://www.kaggle.com/andradaolteanu/sentiment-analysis-rick-and-morty-scripts/#1.-Data-%F0%9F%93%81) by [Andrada Olteanu](https://www.kaggle.com/andradaolteanu) on [Kaggle](https://www.kaggle.com/)\n","\n","\n","By the end of this tutorial you will have your very own chatbot, like the one pictured above üòé\n","\n","Let's get started!\n"]},{"cell_type":"markdown","metadata":{"id":"8EN-zZi6pHIB"},"source":["### Install the Huggingface transformers module"]},{"cell_type":"code","metadata":{"id":"WD6iOcTmoaxE","execution":{"iopub.status.busy":"2021-08-07T14:03:12.875672Z","iopub.execute_input":"2021-08-07T14:03:12.876080Z","iopub.status.idle":"2021-08-07T14:03:19.269243Z","shell.execute_reply.started":"2021-08-07T14:03:12.876045Z","shell.execute_reply":"2021-08-07T14:03:19.268187Z"},"trusted":true},"source":["! pip -q install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IXuRTjrJo5vk"},"source":["## Import DialoGPT\n","DialoGPT is a chatbot model made by microsoft. This will be the base for our RickBot.\n","\n","If you have google collab pro you can use DialoGPT-medium or DialoGPT-large for a more comprehensive bot, otherwise we have to use DialoGPT-small. \\(A meduim or large bot will take longer to download and train\\)"]},{"cell_type":"code","metadata":{"id":"FSvzC1j7_Tr8","execution":{"iopub.status.busy":"2021-08-07T14:03:19.272773Z","iopub.execute_input":"2021-08-07T14:03:19.273052Z","iopub.status.idle":"2021-08-07T14:03:22.767334Z","shell.execute_reply.started":"2021-08-07T14:03:19.273023Z","shell.execute_reply":"2021-08-07T14:03:22.766473Z"},"trusted":true},"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","\n","model_size = \"small\"  # you can change this to \"medium\" or \"large\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")\n","model = AutoModelForCausalLM.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45l8_zjlpD5B"},"source":["## Chat with the untrained model"]},{"cell_type":"code","metadata":{"id":"7NaCfs94pLw4","execution":{"iopub.status.busy":"2021-08-07T14:03:22.769187Z","iopub.execute_input":"2021-08-07T14:03:22.769538Z","iopub.status.idle":"2021-08-07T14:03:36.769613Z","shell.execute_reply.started":"2021-08-07T14:03:22.769502Z","shell.execute_reply":"2021-08-07T14:03:36.768651Z"},"trusted":true},"source":["def chat(model, tokenizer, trained=False):\n","    print(\"type \\\"q\\\" to quit. Automatically quits after 5 messages\")\n","\n","    for step in range(5):\n","        message = input(\"MESSAGE: \")\n","\n","        if message in [\"\", \"q\"]:  # if the user doesn't wanna talk\n","            break\n","\n","        # encode the new user input, add the eos_token and return a tensor in Pytorch\n","        new_user_input_ids = tokenizer.encode(message + tokenizer.eos_token, return_tensors='pt')\n","\n","        # append the new user input tokens to the chat history\n","        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n","\n","        # generated a response while limiting the total chat history to 1000 tokens, \n","        if (trained):\n","            chat_history_ids = model.generate(\n","                bot_input_ids, \n","                max_length=1000,\n","                pad_token_id=tokenizer.eos_token_id,  \n","                no_repeat_ngram_size=3,       \n","                do_sample=True, \n","                top_k=100, \n","                top_p=0.7,\n","                temperature = 0.8,  # 0.6?\n","            )\n","        else:\n","            chat_history_ids = model.generate(\n","                bot_input_ids, \n","                max_length=1000, \n","                pad_token_id=tokenizer.eos_token_id,\n","                no_repeat_ngram_size=3\n","              )\n","\n","        # pretty print last ouput tokens from bot\n","        print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n","\n","chat(model, tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MIF90ucrhgFo"},"source":["It's capable of holding a conversation, but doesn't resemble Rick Sanchez at all yet"]},{"cell_type":"markdown","metadata":{"id":"2Kj2BIaUiS71"},"source":["## Configuring the model"]},{"cell_type":"code","metadata":{"id":"jv9TXRvV1HIk","execution":{"iopub.status.busy":"2021-08-07T14:03:36.771359Z","iopub.execute_input":"2021-08-07T14:03:36.771854Z","iopub.status.idle":"2021-08-07T14:03:36.785617Z","shell.execute_reply.started":"2021-08-07T14:03:36.771820Z","shell.execute_reply":"2021-08-07T14:03:36.784711Z"},"trusted":true},"source":["import glob\n","import logging\n","import os\n","import pickle\n","import random\n","import re\n","import shutil\n","from typing import Dict, List, Tuple\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","\n","from sklearn.model_selection import train_test_split\n","\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm.notebook import tqdm, trange\n","\n","from pathlib import Path\n","\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    PreTrainedModel,\n","    PreTrainedTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","\n","try:\n","    from torch.utils.tensorboard import SummaryWriter\n","except ImportError:\n","    from tensorboardX import SummaryWriter\n","\n","logger = logging.getLogger(__name__)\n","\n","# Args to allow for easy convertion of python script to notebook\n","class Args():\n","    def __init__(self):\n","        self.output_dir = f'output-{model_size}'\n","        self.model_type = 'gpt2'\n","        self.model_name_or_path = f'microsoft/DialoGPT-{model_size}'\n","        self.config_name = f'microsoft/DialoGPT-{model_size}'\n","        self.tokenizer_name = f'microsoft/DialoGPT-{model_size}'\n","        self.cache_dir = 'cached'\n","        self.block_size = 512\n","        self.do_train = True\n","        self.evaluate_during_training = False\n","        self.per_gpu_train_batch_size = 4\n","        self.per_gpu_eval_batch_size = 4\n","        self.gradient_accumulation_steps = 1\n","        self.learning_rate = 5e-5\n","        self.weight_decay = 0.0\n","        self.adam_epsilon = 1e-8\n","        self.max_grad_norm = 1.0\n","        self.num_train_epochs = 3\n","        self.max_steps = -1\n","        self.warmup_steps = 0\n","        self.logging_steps = 1000\n","        self.save_steps = 3500\n","        self.save_total_limit = None\n","        self.eval_all_checkpoints = False\n","        self.no_cuda = False\n","        self.overwrite_output_dir = True\n","        self.overwrite_cache = True\n","        self.should_continue = False\n","        self.seed = 42\n","        self.local_rank = -1\n","\n","args = Args()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aAES6tb6jh1O"},"source":["# Gather the training data\n","\n","We're using some rick and morty scripts from this [article](https://www.kaggle.com/andradaolteanu/sentiment-analysis-rick-and-morty-scripts/#1.-Data-%F0%9F%93%81) by [Andrada Olteanu](https://www.kaggle.com/andradaolteanu)  \\(the data can be found [here](https://www.kaggle.com/andradaolteanu/rickmorty-scripts)\\)"]},{"cell_type":"code","metadata":{"id":"3dS1radujj2J","execution":{"iopub.status.busy":"2021-08-07T14:03:36.787302Z","iopub.execute_input":"2021-08-07T14:03:36.787976Z","iopub.status.idle":"2021-08-07T14:03:38.723873Z","shell.execute_reply.started":"2021-08-07T14:03:36.787921Z","shell.execute_reply":"2021-08-07T14:03:38.722903Z"},"trusted":true},"source":["!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=132JcKBhI89qFQmQ9aeRLbc76-uRu8kCy' -O \"RickAndMortyScripts.csv\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1n_n_Jo1jq4H"},"source":["Let's look at the data:"]},{"cell_type":"code","metadata":{"id":"OFZsicHfjpxz","execution":{"iopub.status.busy":"2021-08-07T14:03:38.725439Z","iopub.execute_input":"2021-08-07T14:03:38.725717Z","iopub.status.idle":"2021-08-07T14:03:38.754588Z","shell.execute_reply.started":"2021-08-07T14:03:38.725675Z","shell.execute_reply":"2021-08-07T14:03:38.753871Z"},"trusted":true},"source":["all_rick = pd.read_csv('RickAndMortyScripts.csv')\n","all_rick.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EKARpNbrjwbf"},"source":["# Converting the training data to a suitable format \n","\n","Although info like the season and episode numbers are interesting, we don't really need them. \n","\n","There's also a few other changes that need to be made before we can train our bot on this data:"]},{"cell_type":"markdown","metadata":{"id":"de1R3f3-kGwY"},"source":["## Adding context\n","We want the model to be aware of previous messages from the dialogue to help it decide what to say next. We call this context"]},{"cell_type":"code","metadata":{"id":"JscMdUEvj3B4","execution":{"iopub.status.busy":"2021-08-07T14:03:38.757206Z","iopub.execute_input":"2021-08-07T14:03:38.757446Z","iopub.status.idle":"2021-08-07T14:03:38.903619Z","shell.execute_reply.started":"2021-08-07T14:03:38.757422Z","shell.execute_reply":"2021-08-07T14:03:38.902792Z"},"trusted":true},"source":["contexted = []\n","\n","n = 7  # You can change this value. A shorter context will train faster but potentially lead to a less informed bot\n","\n","for i in range(n, len(all_rick['line'])):\n","  row = []\n","  prev = i - 1 - n # we additionally substract 1, so row will contain current responce and 7 previous responces  \n","  for j in range(i, prev, -1):\n","    row.append(all_rick['line'][j])\n","  contexted.append(row)  \n","\n","columns = ['response'] + ['context '+ str(i+1) for i in range(n)]\n","\n","df = pd.DataFrame.from_records(contexted, columns=columns)\n","df.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wJw-IgFUv4ir"},"source":["Cool! Now we have context from 7 previous messages"]},{"cell_type":"markdown","metadata":{"id":"5gRD9mhKkcT9"},"source":["### Formatting the data in a way the model can interpret \n","We need the data to be in the right format for the bot to be able to understand it properly. To do this we're adding special characters like the 'end of string' charater to help the bot interpret the dialogue better\n"]},{"cell_type":"code","metadata":{"id":"mdjT5EqKkwZb","execution":{"iopub.status.busy":"2021-08-07T14:03:38.906722Z","iopub.execute_input":"2021-08-07T14:03:38.907102Z","iopub.status.idle":"2021-08-07T14:03:38.917840Z","shell.execute_reply.started":"2021-08-07T14:03:38.907072Z","shell.execute_reply":"2021-08-07T14:03:38.916753Z"},"trusted":true},"source":["def construct_conv(row, tokenizer, eos = True):\n","    flatten = lambda l: [item for sublist in l for item in sublist]\n","    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n","    conv = flatten(conv)\n","    return conv\n","\n","class ConversationDataset(Dataset):\n","    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n","\n","        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n","\n","        directory = args.cache_dir\n","        cached_features_file = os.path.join(\n","            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n","        )\n","\n","        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n","            logger.info(\"Loading features from cached file %s\", cached_features_file)\n","            with open(cached_features_file, \"rb\") as handle:\n","                self.examples = pickle.load(handle)\n","        else:\n","            logger.info(\"Creating features from dataset file at %s\", directory)\n","\n","            self.examples = []\n","            for _, row in df.iterrows():\n","                conv = construct_conv(row, tokenizer)\n","                self.examples.append(conv)\n","\n","            logger.info(\"Saving features into cached file %s\", cached_features_file)\n","            with open(cached_features_file, \"wb\") as handle:\n","                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, item):\n","        return torch.tensor(self.examples[item], dtype=torch.long)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dqiimgaoMhwl"},"source":["We also define some methods to store and load training checkpoints:"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-07T14:03:38.920545Z","iopub.execute_input":"2021-08-07T14:03:38.920970Z","iopub.status.idle":"2021-08-07T14:03:38.934114Z","shell.execute_reply.started":"2021-08-07T14:03:38.920894Z","shell.execute_reply":"2021-08-07T14:03:38.933304Z"},"trusted":true,"id":"NmBvudPVMhwm"},"source":["# Cacheing and storing of data/checkpoints\n","def load_and_cache_examples(args, tokenizer, df_trn):\n","    return ConversationDataset(tokenizer, args, df_trn)\n","\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","\n","def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n","    ordering_and_checkpoint_path = []\n","\n","    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n","\n","    for path in glob_checkpoints:\n","        if use_mtime:\n","            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n","        else:\n","            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n","            if regex_match and regex_match.groups():\n","                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n","\n","    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n","    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n","    return checkpoints_sorted\n","\n","\n","def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n","    if not args.save_total_limit:\n","        return\n","    if args.save_total_limit <= 0:\n","        return\n","\n","    # Check if we should delete older checkpoint(s)\n","    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n","    if len(checkpoints_sorted) <= args.save_total_limit:\n","        return\n","\n","    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n","    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n","    for checkpoint in checkpoints_to_be_deleted:\n","        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n","        shutil.rmtree(checkpoint)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"83oT-xHu4msu"},"source":["# Training\n","\n","Now, this is quite a hefty chunk of code but don't worry you don't need to understant it yet, we can cover this in later tutorials"]},{"cell_type":"code","metadata":{"id":"6W9ZUG-14pI_","execution":{"iopub.status.busy":"2021-08-07T14:03:38.935450Z","iopub.execute_input":"2021-08-07T14:03:38.935954Z","iopub.status.idle":"2021-08-07T14:03:38.969882Z","shell.execute_reply.started":"2021-08-07T14:03:38.935882Z","shell.execute_reply":"2021-08-07T14:03:38.969106Z"},"trusted":true},"source":["def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n","    if args.local_rank in [-1, 0]:\n","        tb_writer = SummaryWriter()\n","\n","    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n","\n","    def collate(examples: List[torch.Tensor]):\n","        if tokenizer._pad_token is None:\n","            return pad_sequence(examples, batch_first=True)\n","        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n","\n","    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n","    train_dataloader = DataLoader(\n","        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n","    )\n","\n","    if args.max_steps > 0:\n","        t_total = args.max_steps\n","        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n","    else:\n","        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n","\n","    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n","    )\n","\n","    # Check if saved optimizer or scheduler states exist\n","    if (\n","        args.model_name_or_path\n","        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n","        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n","    ):\n","        # Load in optimizer and scheduler states\n","        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n","        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n","\n","    # multi-gpu training\n","    if args.n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    # Distributed training\n","    if args.local_rank != -1:\n","        model = torch.nn.parallel.DistributedDataParallel(\n","            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n","        )\n","\n","    logger.info(\"*** Running trainng, Num examples = %d, Num Epochs = %d ***\", len(train_dataset), args.num_train_epochs)\n","\n","    global_step = 0\n","    epochs_trained = 0\n","    steps_trained_in_current_epoch = 0\n","    # Check if continuing training from a checkpoint\n","    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n","        try:\n","            # set global_step to gobal_step of last saved checkpoint from model path\n","            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n","            global_step = int(checkpoint_suffix)\n","            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n","            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n","\n","            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n","            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n","            logger.info(\"  Continuing training from global step %d\", global_step)\n","            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n","        except ValueError:\n","            logger.info(\"  Starting fine-tuning.\")\n","\n","    tr_loss, logging_loss = 0.0, 0.0\n","\n","    model.zero_grad()\n","    train_iterator = trange(\n","        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n","    )\n","    set_seed(args)  # Added here for reproducibility\n","    for _ in train_iterator:\n","        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n","        for step, batch in enumerate(epoch_iterator):\n","\n","            # Skip past any already trained steps if resuming training\n","            if steps_trained_in_current_epoch > 0:\n","                steps_trained_in_current_epoch -= 1\n","                continue\n","\n","            inputs, labels = (batch, batch)\n","            if inputs.shape[1] > 1024: continue\n","            inputs = inputs.to(args.device)\n","            labels = labels.to(args.device)\n","            model.train()\n","            outputs = model(inputs, labels=labels)\n","            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n","\n","            if args.n_gpu > 1:\n","                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            loss.backward()\n","\n","            tr_loss += loss.item()\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","\n","                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n","                    # Log metrics\n","                    if (args.local_rank == -1 and args.evaluate_during_training):  \n","                        # Only evaluate when single GPU otherwise metrics may not average well\n","                        results = evaluate(args, model, tokenizer)\n","                        for key, value in results.items():\n","                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n","                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n","                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n","                    logging_loss = tr_loss\n","\n","                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n","                    checkpoint_prefix = \"checkpoint\"\n","                    # Save model checkpoint\n","                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n","                    os.makedirs(output_dir, exist_ok=True)\n","                    \n","                    # Take care of distributed/parallel training\n","                    model_to_save = (model.module if hasattr(model, \"module\") else model)  \n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(output_dir)\n","\n","                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n","                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n","\n","                    _rotate_checkpoints(args, checkpoint_prefix)\n","\n","                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n","                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n","                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n","\n","            if args.max_steps > 0 and global_step > args.max_steps:\n","                epoch_iterator.close()\n","                break\n","        if args.max_steps > 0 and global_step > args.max_steps:\n","            train_iterator.close()\n","            break\n","\n","    if args.local_rank in [-1, 0]:\n","        tb_writer.close()\n","\n","    return global_step, tr_loss / global_step"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vWjTu6fI4yP8"},"source":["# Main Runner\n","\n","Here we're simply setting up the logger and starting the training!"]},{"cell_type":"code","metadata":{"id":"Jludg4aN4zdc","execution":{"iopub.status.busy":"2021-08-07T14:03:38.971342Z","iopub.execute_input":"2021-08-07T14:03:38.971807Z","iopub.status.idle":"2021-08-07T14:03:38.987706Z","shell.execute_reply.started":"2021-08-07T14:03:38.971770Z","shell.execute_reply":"2021-08-07T14:03:38.986681Z"},"trusted":true},"source":["def main(df_trn):\n","    args = Args()\n","    \n","    if args.should_continue:\n","        sorted_checkpoints = _sorted_checkpoints(args)\n","        if len(sorted_checkpoints) == 0:\n","            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n","        else:\n","            args.model_name_or_path = sorted_checkpoints[-1]\n","\n","    if (os.path.exists(args.output_dir)\n","        and os.listdir(args.output_dir)\n","        and args.do_train\n","        and not args.overwrite_output_dir\n","        and not args.should_continue\n","    ):\n","        raise ValueError(\n","            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n","                args.output_dir\n","            )\n","        )\n","\n","    # Setup CUDA, GPU & distributed training\n","    device = torch.device(\"cuda\")\n","    args.n_gpu = torch.cuda.device_count()\n","    args.device = device\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s\",\n","        args.local_rank,\n","        device,\n","        args.n_gpu,\n","        bool(args.local_rank != -1)\n","    )\n","\n","    set_seed(args) # Set seed\n","\n","    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n","    model = AutoModelForCausalLM.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=False,\n","        config=config,\n","        cache_dir=args.cache_dir,\n","    )\n","    model.to(args.device)\n","    \n","    logger.info(\"Training parameters %s\", args)\n","\n","    # Training\n","    if args.do_train:\n","        train_dataset = load_and_cache_examples(args, tokenizer, df_trn)\n","        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n","        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n","\n","    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n","    if args.do_train:\n","        # Create output directory if needed\n","        os.makedirs(args.output_dir, exist_ok=True)\n","\n","        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n","        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","        # They can then be reloaded using `from_pretrained()`\n","        model_to_save = (model.module if hasattr(model, \"module\") else model)  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","\n","        # Good practice: save your training arguments together with the trained model\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","\n","        # Load a trained model and vocabulary that you have fine-tuned\n","        model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n","        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n","        model.to(args.device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ApbF-p305CYv"},"source":["# Lets Run it!\n","This should take around 10 minutes so you might as well go grab a cup of coffee ‚òïÔ∏è"]},{"cell_type":"code","metadata":{"id":"sfTdpQy-5D1n","execution":{"iopub.status.busy":"2021-08-07T14:03:38.989335Z","iopub.execute_input":"2021-08-07T14:03:38.989811Z","iopub.status.idle":"2021-08-07T14:08:00.933974Z","shell.execute_reply.started":"2021-08-07T14:03:38.989772Z","shell.execute_reply":"2021-08-07T14:08:00.933072Z"},"trusted":true},"source":["main(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4xYlHoEB5Jic"},"source":["# Chatting with the trained bot"]},{"cell_type":"code","metadata":{"id":"NkZ0yjsc5LX-","execution":{"iopub.status.busy":"2021-08-07T14:25:59.805297Z","iopub.execute_input":"2021-08-07T14:25:59.805625Z","iopub.status.idle":"2021-08-07T14:26:35.558955Z","shell.execute_reply.started":"2021-08-07T14:25:59.805593Z","shell.execute_reply":"2021-08-07T14:26:35.557929Z"},"trusted":true},"source":["tokenizer = AutoTokenizer.from_pretrained(f'microsoft/DialoGPT-{model_size}')\n","model = AutoModelForCausalLM.from_pretrained(f'output-{model_size}')\n","    \n","chat(model, tokenizer, trained=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_vFiJlFZwuPF"},"source":["That's more like it!"]}]}